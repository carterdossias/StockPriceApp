{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GETTING STOCK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully into the MySQL database.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def safe_float(value):\n",
    "    \"\"\"Convert a value to float if not NaN, otherwise return None.\"\"\"\n",
    "    if pd.isnull(value):\n",
    "        return None\n",
    "    return float(value)\n",
    "\n",
    "def main():\n",
    "    # Database connection parameters\n",
    "    server = '192.168.0.17'\n",
    "    username = 'admin'\n",
    "    password = 'spotify'\n",
    "    database = 'Stocks_DB'\n",
    "    \n",
    "    # Establish connection to MySQL\n",
    "    conn = mysql.connector.connect(\n",
    "        host=server,\n",
    "        user=username,\n",
    "        password=password,\n",
    "        database=database\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Define the ticker symbol\n",
    "    ticker_symbol = 'DIS'\n",
    "    \n",
    "    # Fetch historical stock data using yfinance\n",
    "    stock = yf.Ticker(ticker_symbol)\n",
    "    hist = stock.history(period=\"max\")\n",
    "    \n",
    "    # Localize the index to UTC to bypass DST-related issues\n",
    "    if hist.index.tzinfo is None:\n",
    "        hist.index = hist.index.tz_localize('UTC', nonexistent='shift_forward', ambiguous='NaT')\n",
    "    else:\n",
    "        hist.index = hist.index.tz_convert('UTC')\n",
    "    \n",
    "    # Create table name based on ticker symbol (e.g., DIS_data)\n",
    "    table_name = f\"{ticker_symbol}_data\"\n",
    "    \n",
    "    # SQL to create table if it doesn't exist (note: removed trailing comma)\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        date DATE PRIMARY KEY,\n",
    "        open FLOAT,\n",
    "        high FLOAT,\n",
    "        low FLOAT,\n",
    "        close FLOAT,\n",
    "        volume BIGINT\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    \n",
    "    # SQL to insert data into the table with upsert functionality\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (date, open, high, low, close, volume)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        open = VALUES(open),\n",
    "        high = VALUES(high),\n",
    "        low = VALUES(low),\n",
    "        close = VALUES(close),\n",
    "        volume = VALUES(volume)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate over DataFrame rows and insert into the table\n",
    "    for index, row in hist.iterrows():\n",
    "        data_tuple = (\n",
    "            index.date(), \n",
    "            safe_float(row.get('Open')), \n",
    "            safe_float(row.get('High')), \n",
    "            safe_float(row.get('Low')), \n",
    "            safe_float(row.get('Close')), \n",
    "            int(row.get('Volume')) if not pd.isnull(row.get('Volume')) else None\n",
    "        )\n",
    "        cursor.execute(insert_query, data_tuple)\n",
    "    \n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Data imported successfully into the MySQL database.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script that inserts News DIRECTLY into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for 2024-01-01 to 2024-01-07 for TSLA\n",
      "Fetching news for 2024-01-08 to 2024-01-14 for TSLA\n",
      "Fetching news for 2024-01-15 to 2024-01-21 for TSLA\n",
      "Fetching news for 2024-01-22 to 2024-01-28 for TSLA\n",
      "Fetching news for 2024-01-29 to 2024-02-04 for TSLA\n",
      "Fetching news for 2024-02-05 to 2024-02-11 for TSLA\n",
      "Fetching news for 2024-02-12 to 2024-02-18 for TSLA\n",
      "Fetching news for 2024-02-19 to 2024-02-25 for TSLA\n",
      "Fetching news for 2024-02-26 to 2024-03-03 for TSLA\n",
      "Fetching news for 2024-03-04 to 2024-03-10 for TSLA\n",
      "Fetching news for 2024-03-11 to 2024-03-17 for TSLA\n",
      "Fetching news for 2024-03-18 to 2024-03-24 for TSLA\n",
      "Fetching news for 2024-03-25 to 2024-03-31 for TSLA\n",
      "Fetching news for 2024-04-01 to 2024-04-07 for TSLA\n",
      "Fetching news for 2024-04-08 to 2024-04-14 for TSLA\n",
      "Fetching news for 2024-04-15 to 2024-04-21 for TSLA\n",
      "Fetching news for 2024-04-22 to 2024-04-28 for TSLA\n",
      "Fetching news for 2024-04-29 to 2024-05-05 for TSLA\n",
      "Fetching news for 2024-05-06 to 2024-05-12 for TSLA\n",
      "Fetching news for 2024-05-13 to 2024-05-19 for TSLA\n",
      "Fetching news for 2024-05-20 to 2024-05-26 for TSLA\n",
      "Fetching news for 2024-05-27 to 2024-06-02 for TSLA\n",
      "Fetching news for 2024-06-03 to 2024-06-09 for TSLA\n",
      "Fetching news for 2024-06-10 to 2024-06-16 for TSLA\n",
      "Fetching news for 2024-06-17 to 2024-06-23 for TSLA\n",
      "Fetching news for 2024-06-24 to 2024-06-30 for TSLA\n",
      "Fetching news for 2024-07-01 to 2024-07-07 for TSLA\n",
      "Fetching news for 2024-07-08 to 2024-07-14 for TSLA\n",
      "Fetching news for 2024-07-15 to 2024-07-21 for TSLA\n",
      "Fetching news for 2024-07-22 to 2024-07-28 for TSLA\n",
      "Fetching news for 2024-07-29 to 2024-08-04 for TSLA\n",
      "Fetching news for 2024-08-05 to 2024-08-11 for TSLA\n",
      "Fetching news for 2024-08-12 to 2024-08-18 for TSLA\n",
      "Fetching news for 2024-08-19 to 2024-08-25 for TSLA\n",
      "Fetching news for 2024-08-26 to 2024-09-01 for TSLA\n",
      "Fetching news for 2024-09-02 to 2024-09-08 for TSLA\n",
      "Fetching news for 2024-09-09 to 2024-09-15 for TSLA\n",
      "Fetching news for 2024-09-16 to 2024-09-22 for TSLA\n",
      "Fetching news for 2024-09-23 to 2024-09-29 for TSLA\n",
      "Fetching news for 2024-09-30 to 2024-10-06 for TSLA\n",
      "Fetching news for 2024-10-07 to 2024-10-13 for TSLA\n",
      "Fetching news for 2024-10-14 to 2024-10-20 for TSLA\n",
      "Fetching news for 2024-10-21 to 2024-10-27 for TSLA\n",
      "Fetching news for 2024-10-28 to 2024-11-03 for TSLA\n",
      "Fetching news for 2024-11-04 to 2024-11-10 for TSLA\n",
      "Fetching news for 2024-11-11 to 2024-11-17 for TSLA\n",
      "Fetching news for 2024-11-18 to 2024-11-24 for TSLA\n",
      "Fetching news for 2024-11-25 to 2024-12-01 for TSLA\n",
      "Fetching news for 2024-12-02 to 2024-12-08 for TSLA\n",
      "Fetching news for 2024-12-09 to 2024-12-15 for TSLA\n",
      "Fetching news for 2024-12-16 to 2024-12-22 for TSLA\n",
      "Fetching news for 2024-12-23 to 2024-12-29 for TSLA\n",
      "Fetching news for 2024-12-30 to 2025-01-05 for TSLA\n",
      "Fetching news for 2025-01-06 to 2025-01-12 for TSLA\n",
      "Fetching news for 2025-01-13 to 2025-01-19 for TSLA\n",
      "Fetching news for 2025-01-20 to 2025-01-26 for TSLA\n",
      "Fetching news for 2025-01-27 to 2025-02-02 for TSLA\n",
      "Fetching news for 2025-02-03 to 2025-02-09 for TSLA\n",
      "Fetching news for 2025-02-10 to 2025-02-16 for TSLA\n",
      "Fetching news for 2025-02-17 to 2025-02-23 for TSLA\n",
      "Fetching news for 2025-02-24 to 2025-03-02 for TSLA\n",
      "Fetching news for 2025-03-03 to 2025-03-09 for TSLA\n",
      "Inserted 10123 news articles into TSLA_news table.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "API_KEY = \"cv6gs71r01qi7f6qb7kgcv6gs71r01qi7f6qb7l0\"\n",
    "BASE_URL = \"https://finnhub.io/api/v1/company-news\"\n",
    "\n",
    "# Replace with any stock ticker\n",
    "ticker = \"TSLA\"\n",
    "\n",
    "# Database Configuration\n",
    "db_config = {\n",
    "    'host': '192.168.0.17',  # Change this if needed\n",
    "    'user': 'admin',\n",
    "    'password': 'spotify',\n",
    "    'database': 'Stocks_DB'\n",
    "}\n",
    "\n",
    "# Define the start date and end date (today)\n",
    "start_date = datetime.date(2024, 1, 1)\n",
    "end_date = datetime.date.today()\n",
    "\n",
    "all_news = []\n",
    "current_start = start_date\n",
    "\n",
    "# Maximum number of retries for a 429 error (rate limit)\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# Establish database connection\n",
    "conn = mysql.connector.connect(**db_config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table if not exists\n",
    "create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {ticker}_news (\n",
    "        news_id BIGINT PRIMARY KEY,\n",
    "        date_time DATE,\n",
    "        headline TEXT,\n",
    "        related VARCHAR(10),\n",
    "        source_ VARCHAR(255),\n",
    "        summary TEXT,\n",
    "        sentiment DOUBLE\n",
    "    );\n",
    "\"\"\"\n",
    "cursor.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "while current_start <= end_date:\n",
    "    current_end = current_start + datetime.timedelta(days=6)\n",
    "    if current_end > end_date:\n",
    "        current_end = end_date\n",
    "\n",
    "    params = {\n",
    "        \"symbol\": ticker,\n",
    "        \"from\": current_start.strftime(\"%Y-%m-%d\"),\n",
    "        \"to\": current_end.strftime(\"%Y-%m-%d\"),\n",
    "        \"token\": API_KEY\n",
    "    }\n",
    "\n",
    "    print(f\"Fetching news for {params['from']} to {params['to']} for {ticker}\")\n",
    "\n",
    "    retries = 0\n",
    "    success = False\n",
    "    while not success and retries < MAX_RETRIES:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        if response.status_code == 200:\n",
    "            news_items = response.json()\n",
    "            if news_items:\n",
    "                all_news.extend(news_items)\n",
    "            success = True\n",
    "        elif response.status_code == 429:\n",
    "            retries += 1\n",
    "            wait_time = 2 ** retries\n",
    "            print(f\"Rate limit reached. Retrying in {wait_time} seconds (attempt {retries}/{MAX_RETRIES})\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} for range {params['from']} to {params['to']}\")\n",
    "            success = True  # Exit retry loop on non-429 errors\n",
    "\n",
    "    current_start = current_end + datetime.timedelta(days=1)\n",
    "    time.sleep(1)  # Helps avoid rapid-fire requests\n",
    "\n",
    "# Convert collected news data to DataFrame\n",
    "df = pd.DataFrame(all_news)\n",
    "\n",
    "# Helper function to safely convert Unix timestamp to \"YYYY-MM-DD\"\n",
    "def safe_convert(ts):\n",
    "    try:\n",
    "        ts_val = int(ts)\n",
    "        if ts_val <= 0:\n",
    "            return None\n",
    "        return datetime.datetime.fromtimestamp(ts_val).date()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# If the 'datetime' column exists, convert timestamps\n",
    "if not df.empty and 'datetime' in df.columns:\n",
    "    df['datetime'] = df['datetime'].apply(safe_convert)\n",
    "\n",
    "# Insert data into MySQL database\n",
    "insert_query = f\"\"\"\n",
    "    INSERT INTO {ticker}_news (news_id, date_time, headline, related, source_, summary)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    ON DUPLICATE KEY UPDATE \n",
    "        date_time = VALUES(date_time),\n",
    "        headline = VALUES(headline),\n",
    "        related = VALUES(related),\n",
    "        source_ = VALUES(source_),\n",
    "        summary = VALUES(summary);\n",
    "\"\"\"\n",
    "\n",
    "rows_inserted = 0\n",
    "for _, row in df.iterrows():\n",
    "    if pd.isna(row.get('id')) or pd.isna(row.get('datetime')):  # Skip if no ID or Date\n",
    "        continue\n",
    "    data = (\n",
    "        int(row['id']), row['datetime'], row.get('headline', ''),\n",
    "        row.get('related', ''), row.get('source', ''), row.get('summary', '')\n",
    "    )\n",
    "    cursor.execute(insert_query, data)\n",
    "    rows_inserted += 1\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {rows_inserted} news articles into {ticker}_news table.\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD GETTING NEWS WITH FINNHUB API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for 2025-01-01 to 2025-01-07\n",
      "Fetching news for 2025-01-08 to 2025-01-14\n",
      "Fetching news for 2025-01-15 to 2025-01-21\n",
      "Fetching news for 2025-01-22 to 2025-01-28\n",
      "Fetching news for 2025-01-29 to 2025-02-04\n",
      "Fetching news for 2025-02-05 to 2025-02-11\n",
      "Fetching news for 2025-02-12 to 2025-02-18\n",
      "Fetching news for 2025-02-19 to 2025-02-25\n",
      "Fetching news for 2025-02-26 to 2025-03-04\n",
      "Fetching news for 2025-03-05 to 2025-03-09\n",
      "All news data saved to AAPL_all_news_weekly.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Finnhub API key and endpoint details\n",
    "API_KEY = \"cv6gs71r01qi7f6qb7kgcv6gs71r01qi7f6qb7l0\"\n",
    "BASE_URL = \"https://finnhub.io/api/v1/company-news\"\n",
    "symbol = \"AAPL\"\n",
    "\n",
    "# Define the start date (adjust as needed) and end date (today)\n",
    "start_date = datetime.date(2025, 1, 1)  # Example start date\n",
    "end_date = datetime.date.today()\n",
    "\n",
    "all_news = []\n",
    "current_start = start_date\n",
    "\n",
    "# Maximum number of retries for a 429 error\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "while current_start <= end_date:\n",
    "    # Define a weekly window: from current_start to current_start + 6 days\n",
    "    current_end = current_start + datetime.timedelta(days=6)\n",
    "    if current_end > end_date:\n",
    "        current_end = end_date\n",
    "\n",
    "    # Format the dates as YYYY-MM-DD strings\n",
    "    params = {\n",
    "        \"symbol\": symbol,\n",
    "        \"from\": current_start.strftime(\"%Y-%m-%d\"),\n",
    "        \"to\": current_end.strftime(\"%Y-%m-%d\"),\n",
    "        \"token\": API_KEY\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetching news for {params['from']} to {params['to']}\")\n",
    "    \n",
    "    retries = 0\n",
    "    success = False\n",
    "    while not success and retries < MAX_RETRIES:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        if response.status_code == 200:\n",
    "            news_items = response.json()\n",
    "            if news_items:\n",
    "                all_news.extend(news_items)\n",
    "            success = True\n",
    "        elif response.status_code == 429:\n",
    "            # Rate limit error: wait longer and then retry\n",
    "            retries += 1\n",
    "            wait_time = 2 ** retries  # exponential backoff\n",
    "            print(f\"Rate limit reached. Retrying in {wait_time} seconds (attempt {retries}/{MAX_RETRIES})\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} for range {params['from']} to {params['to']}\")\n",
    "            success = True  # exit retry loop on non-429 errors\n",
    "\n",
    "    # Move to the next week\n",
    "    current_start = current_end + datetime.timedelta(days=1)\n",
    "    # Sleep briefly to help avoid rapid-fire requests\n",
    "    time.sleep(1)\n",
    "\n",
    "# Convert the collected news data to a DataFrame\n",
    "df = pd.DataFrame(all_news)\n",
    "\n",
    "# Helper function to safely convert Unix timestamp to \"YYYY-MM-DD\"\n",
    "def safe_convert(ts):\n",
    "    try:\n",
    "        ts_val = int(ts)\n",
    "        if ts_val <= 0:\n",
    "            return \"\"\n",
    "        return datetime.datetime.fromtimestamp(ts_val).strftime(\"%Y-%m-%d\")\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "# If the 'datetime' column exists, convert the Unix timestamp to \"YYYY-MM-DD\"\n",
    "if not df.empty and 'datetime' in df.columns:\n",
    "    df['datetime'] = df['datetime'].apply(safe_convert)\n",
    "\n",
    "csv_filename = \"AAPL_all_news_weekly.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"All news data saved to {csv_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
